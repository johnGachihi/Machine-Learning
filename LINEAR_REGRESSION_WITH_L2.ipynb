{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LINEAR REGRESSION WITH L2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYxVQo1pRctmMTzfIccQjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnGachihi/linear-regression-with-generalization/blob/master/LINEAR_REGRESSION_WITH_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nieofgm0TzAH",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression with L2:\n",
        "\n",
        "\\begin{align*}\n",
        "  f(m, c) &= \\frac{1}{N}\\sum_{i=1}^{N} [y_i - (mx_i + c)]^2 - \\lambda \\sum_{j=1}^K ||m||^2_2\n",
        "  \\\\\n",
        "  &= \\frac{1}{N}\\sum_{i=1}^{N} [y_i - (mx_i + c)]^2 - \\lambda \\sum_{j=1}^K m^2\n",
        "  \\\\\n",
        "  \\\\\n",
        "  \\frac{\\partial f(m,c)}{\\partial m} &= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial}{\\partial m}[y_i - (mx_i + c)]^2 - \\lambda \\sum_{j=1}^K \\frac{\\partial}{\\partial m} m^2 &\n",
        "  (m\\text{ is a scalar therefore we can remove the summation})\n",
        "  \\\\\n",
        "  &= -\\frac{2}{N} \\sum_{i=1}^{N} x_i[y_i - (mx_i + c)] - 2\\lambda m & \\text{(Our new D_m)}\n",
        "  \\\\\n",
        "  \\\\\n",
        "  \\frac{\\partial f(m,c)}{\\partial c} &= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial}{\\partial c}[y_i - (mx_i + c)]^2 - \\lambda \\sum_{j=1}^K \\frac{\\partial}{\\partial c} m^2\n",
        "  \\\\\n",
        "  &= -\\frac{2}{N} \\sum_{i=1}^{N} [y_i - (mx_i + c)] - 0 & \\text{(Our D_c does not change)}\n",
        "  \\\\\n",
        "  \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mOTr48nQh_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LINEAR REGRESSION WITH L2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "dataset = pd.read_csv(\"data.csv\", header=0, names=[\"size\", \"price\"])\n",
        "X = dataset[\"size\"]\n",
        "Y = dataset[\"price\"]\n",
        "N = X.size\n",
        "L = 0.0001\n",
        "lmbd=1.2\n",
        "\n",
        "def mse(N, Y, Y_hat):\n",
        "  return (1/N) * sum((Y-Y_hat)**2)\n",
        "\n",
        "def gradient_descent_fit(X, Y):\n",
        "  m = 0 # Y = mX + c\n",
        "  c = 0\n",
        "  epoch_error_list = []\n",
        "\n",
        "#  for (int i = 0; i < 100; i++)\n",
        "  for i in range(N):\n",
        "    Y_hat = m*X + c\n",
        "    D_m = (-2/N) * sum(X * (Y-Y_hat)) - 2*lmbd*m\n",
        "    m = m - L*D_m\n",
        "    D_c = (-2/N) * sum(Y-Y_hat)\n",
        "    c = c - L*D_c\n",
        "\n",
        "    epoch_error_list.append(mse(N, Y, Y_hat))\n",
        "\n",
        "  return Y_hat, epoch_error_list\n",
        "\n",
        "(Y_hat, epoch_error_list) = gradient_descent_fit(X, Y)\n",
        "\n",
        "_, (best_fit_axes, error_axes) = plt.subplots(2)\n",
        "\n",
        "best_fit_axes.scatter(X, Y)\n",
        "best_fit_axes.plot([min(X), max(X)], [min(Y_hat), max(Y_hat)])\n",
        "\n",
        "error_axes.plot(range(N), epoch_error_list)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}